{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fdf311",
   "metadata": {},
   "source": [
    "# Basic RAG Pipeline Implementation\n",
    "\n",
    "\n",
    "### Intro\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline using classes from src directory. The implementation is divided into individual pipeline stages:\n",
    "\n",
    "\n",
    "1. Document Processing: Token-aware chunking with overlap\n",
    "2. Embedding Generation: Semantic vectorization\n",
    "3. Vector Storage: PostgreSQL + pgvector indexing\n",
    "4. Retrieval: Cosine similarit*y search\n",
    "5. Generation: Context-augmented LLM completion\n",
    "\n",
    "### Components\n",
    "\n",
    "**TextProcessor** - Text segmentation and token budget management.\n",
    "\n",
    "* Token-based chunking (512 tokens, 50 token overlap)\n",
    "* Uses cl100k_base tokenizer for GPT compatibility\n",
    "* Adaptive context assembly within token budgets\n",
    "\n",
    "**HuggingFaceClient** - Embedding generation and LLM inference.\n",
    "\n",
    "* Embeddings: Local sentence-transformers (all-MiniLM-L6-v2, 384-dim)\n",
    "* Generation: Remote Hugging Face Inference API (default: Mistral-7B-Instruct)\n",
    "\n",
    "**PgVectorDB** - PostgreSQL interface with vector similarity search.\n",
    "\n",
    "* Stores embeddings as VECTOR(384) with chunk metadata\n",
    "* Uses ivfflat indexing for approximate nearest neighbor search\n",
    "* Cosine similarity search via <=> operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d5c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Optional\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# import psycopg2\n",
    "# from psycopg2.extras import execute_values, Json\n",
    "# from pgvector.psycopg2 import register_vector\n",
    "# from huggingface_hub import InferenceClient\n",
    "# from transformers import pipeline\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc959ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processor import TextProcessor\n",
    "from pgvector_client import PgVectorClient\n",
    "from hf_client import HuggingFaceClient\n",
    "# from rag_handler import PgVectorRAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19e7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_CONN_STRING = os.getenv(\"PG_CONNECTION_STRING\")\n",
    "HF_TOKEN= os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "file_paths = [\n",
    "    \"../documents/policy.txt\",\n",
    "    \"../documents/basic_info.md\",\n",
    "    # Add more files\n",
    "]\n",
    "\n",
    "batch_size=BATCH_SIZE=32\n",
    "\n",
    "#  Query the system\n",
    "questions = [\n",
    "    \"What is mario's email?\",\n",
    "    \"How long does shipping take?\",\n",
    "    \"Where there any projects with recommendation systems done by Mario?\",\n",
    "    \"Does mario like data science?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4d786ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pgvector_client:Database connected\n",
      "INFO:pgvector_client:pgvector extension enabled\n",
      "INFO:pgvector_client:Database schema created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf_client:Initialized embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:hf_client:Initialized LLM: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# Initialize components \n",
    "text_processor = TextProcessor(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    max_context_tokens=2000\n",
    ")\n",
    "\n",
    "db_client = PgVectorClient(\n",
    "    connection_string=PG_CONN_STRING,\n",
    "    embedding_dim=384  # Match embedding model output\n",
    ")\n",
    "\n",
    "hf_client = HuggingFaceClient(\n",
    "    hf_token=HF_TOKEN,\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    llm_model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    # use_remote_llm=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a56b28",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae05a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:text_processor:Chunked policy.txt: 1 chunks\n",
      "INFO:text_processor:Chunked basic_info.md: 2 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks = []    \n",
    "for file_path in file_paths:        \n",
    "    try:            \n",
    "        file_chunks = text_processor.chunk_file(file_path)            \n",
    "        chunks.extend(file_chunks)        \n",
    "    except Exception as e:            \n",
    "        logger.error(f\"Failed to chunk {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e9bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:chunk number 0, chunk len 107: {'content': 'Our refund policy allows returns within 35 days of purchase. Full refunds are provided for unopened items.\\n', 'chunk_id': 0, 'token_count': 21, 'start_token': 0, 'end_token': 21, 'source': 'policy.txt'}\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:chunk number 1, chunk len 2646: {'content': '# Cover letter\\n\\n## Basic info\\n\\n\\nThis is a document with CV summary\\n\\n\\nName: Mario (Marvin) Theplumber\\n\\nemail: mario.thplumber@gmail.com\\nphone: +72787226083\\n\\nNorth Holland, Netherlands\\nhttps://github.com/razmarrus\\nhttps://www.linkedin.com/in/razmarrus/\\n\\nnpm i jsonresume-theme-caffeine-tweaked\\nresume export --theme caffeine-tweaked resume.pdf\\n\\nadress: Beethovenstraat 22-1 1099 LK Rotterdam\\nNetherlands \\n\\nEducation: Brazil State University of Informatics\\n\\n\\n## About me / Role description\\n\\nAs a results-oriented Data Scientist, I believe that combining Machine Learning and Statistics provides businesses with solid answers to their questions. While staying close to the data, I work closely with my business-oriented colleagues to translate business goals into achievable objectives using models.\\n\\nMy focus is on customer and financial analytics, where I assist in improving marketing strategies with a better understanding of customers’ preferences and measuring the effectiveness of promotion with sales prediction. \\n\\nThe outcomes of my work are often presented as regularly updated dashboards and sheets.\\n\\nMy expertise encompasses the full pipeline delivery, from extracting raw data from the warehouse and creating datasets to tuning models and automating updates. Since data investigation and model tuning are iterative processes, I regularly present my findings and progress using clear and concise slides. Knowledgeable in optimizing and updating old models for new requirements, followed by integration into the client’s infrastructure.\\n\\n\\nI do love Data Science because the process of brainstorming new data is quite fun.\\nI see the DS work as an endless process of testing your own ideas. Of course, there are know-how and\\ncommon approaches, but you still must think and improvise to get better results.\\nThat is why data scientist in consulting is a perfect match for me\\n\\nCurrently I live in Amsterdam, while working remotely. I have Netherlands residence permit, so i don’t need to make\\nany additional paperwork to work in Netherlands\\n\\n## Recommendation system\\n \\n I developed a multi-channel recommendation system for a large pharmaceutical client to optimize marketing strategies across 4 communication channels: email, WhatsApp, call-center, and in-person meetings. \\nFour separate ML models were created, each required an individual business approach to define what constituted a successful communication. I handled the project end-to-end - from extracting data from client databases, to creating and scheduling Boosting (LightGBM) models in Airflow, and delivering results back to the client’s systems. Models were updated monthly. \\n\\nI', 'chunk_id': 0, 'token_count': 512, 'start_token': 0, 'end_token': 512, 'source': 'basic_info.md'}\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:chunk number 2, chunk len 643: {'content': ' communication. I handled the project end-to-end - from extracting data from client databases, to creating and scheduling Boosting (LightGBM) models in Airflow, and delivering results back to the client’s systems. Models were updated monthly. \\n\\nI investigated each data source to identify key factors driving engagement and worked closely with a business analyst and project manager to align the solution with business goals. \\nThe results from all models were combined into one system to provide personalized recommendations for each channel over the next three months. It created a table with communication suggestions for every individual.\\n\\n', 'chunk_id': 1, 'token_count': 113, 'start_token': 462, 'end_token': 575, 'source': 'basic_info.md'}\n",
      "INFO:__main__:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ind, chunk in enumerate(chunks):\n",
    "    logger.info(f\"chunk number {ind}, chunk len {len(chunk['content'])}: {chunk}\")\n",
    "    logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e0e1005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf_client:Generated 3 embeddings\n",
      "INFO:__main__:Embedded batch 1/1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Generate embeddings in batches (HuggingFaceClient)\n",
    "texts = [chunk[\"content\"] for chunk in chunks]\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    batch_embeddings = hf_client.get_embeddings(batch)\n",
    "    embeddings.extend(batch_embeddings)   # adds each embedding vector\n",
    "    logger.info(f\"Embedded batch {i // batch_size + 1}/{(len(texts) + batch_size - 1) // batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pgvector_client:Inserted 3 chunks\n"
     ]
    }
   ],
   "source": [
    "# Insert into database (PgVectorDB)\n",
    "db_client.insert_chunks(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f497e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Log statistics (TextProcessor)\n",
    "# stats = text_processor.get_chunk_stats(chunks)\n",
    "# logger.info(\n",
    "#     f\"Loaded {stats['total_chunks']} chunks, \"\n",
    "#     f\"avg {stats['avg_tokens']:.0f} tokens/chunk\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d616b",
   "metadata": {},
   "source": [
    "## Query the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6f9798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question, text_processor, hf_client, db_client, k=5):\n",
    "    \"\"\"\n",
    "    Execute RAG query: embed → search → assemble context → generate answer.\n",
    "    \n",
    "    Args:\n",
    "        question: User query\n",
    "        text_processor: TextProcessor instance\n",
    "        hf_client: HuggingFaceClient instance\n",
    "        db_client: PgVectorDB instance\n",
    "        k: Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        dict: {\"answer\": str, \"sources\": list, \"num_chunks\": int}\n",
    "    \"\"\"\n",
    "    # 1. Embed question\n",
    "    query_embedding = hf_client.get_embeddings([question])[0]\n",
    "    \n",
    "    # 2. Search database\n",
    "    chunks = db_client.search(query_embedding, k=k)\n",
    "    \n",
    "    if not chunks:\n",
    "        return {\"answer\": \"No relevant information found.\", \"sources\": [], \"num_chunks\": 0}\n",
    "    \n",
    "    # 3. Assemble context\n",
    "    context = text_processor.assemble_context(chunks, question=question)\n",
    "    \n",
    "    # 4. Generate answer (with fallback)\n",
    "    try:\n",
    "        answer = hf_client.generate_answer(question, context)\n",
    "        if not answer or len(answer) < 10:\n",
    "            answer = text_processor.create_fallback(chunks)\n",
    "    except:\n",
    "        answer = text_processor.create_fallback(chunks)\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [c.get(\"text\", \"\") for c in chunks],\n",
    "        \"num_chunks\": len(chunks)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_rag(\n",
    "    question=\"What is RAG?\",\n",
    "    text_processor=text_processor,\n",
    "    hf_client=hf_client,\n",
    "    db_client=db_client,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14120f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d936d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question: What is mario's email?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     result = \u001b[43mrag\u001b[49m.query(\n\u001b[32m     13\u001b[39m         question=question,\n\u001b[32m     14\u001b[39m         k=\u001b[32m5\u001b[39m,\n\u001b[32m     15\u001b[39m         adaptive_context=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSources (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mnum_chunks\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'rag' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n Question: {question}\")\n",
    "    \n",
    "    result = query_rag(\n",
    "        question=question,\n",
    "        k=5,\n",
    "        adaptive_context=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "    print(f\"\\nSources ({result['num_chunks']} chunks):\")\n",
    "    for src in result['sources']:\n",
    "        print(\n",
    "            f\"  - {src['source']} (chunk {src['chunk_id']}, \"\n",
    "            f\"tokens {src['start_token']}-{src['end_token']}, \"\n",
    "            f\"score {src['similarity']})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Generate query embedding (HuggingFaceClient)\n",
    "# query_embedding = self.hf.get_embeddings([question])[0]\n",
    "\n",
    "# # 2. Search database (PgVectorDB)\n",
    "# chunks = self.db.search(\n",
    "#     query_embedding,\n",
    "#     k=k,\n",
    "#     similarity_threshold=similarity_threshold\n",
    "# )\n",
    "\n",
    "# if not chunks:\n",
    "#     return {\n",
    "#         \"answer\": \"No relevant information found.\",\n",
    "#         \"sources\": [],\n",
    "#         \"num_chunks\": 0,\n",
    "#         \"token_usage\": {\"context\": 0, \"question\": 0}\n",
    "#     }\n",
    "\n",
    "# # 3. Assemble context (TextProcessor)\n",
    "# if adaptive_context:\n",
    "#     context = self.text.assemble_context(chunks, question=question)\n",
    "# else:\n",
    "#     context = self.text.assemble_context(chunks)\n",
    "\n",
    "# # 4. Generate answer (HuggingFaceClient) with fallback (TextProcessor)\n",
    "# try:\n",
    "#     answer = self.hf.generate_answer(question, context)\n",
    "#     if not answer or len(answer) < 10:\n",
    "#         logger.warning(\"LLM returned empty/short answer, using fallback\")\n",
    "#         answer = self.text.create_fallback(chunks)\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Generation failed: {e}, using fallback\")\n",
    "#     answer = self.text.create_fallback(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_cv_v2",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
